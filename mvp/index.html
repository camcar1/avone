<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Avatar MVP</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/GLTFLoader.js"></script>
    <style>
        * {
            box-sizing: border-box;
        }
        
        body {
            margin: 0;
            padding: 0;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(45deg, #1e3c72, #2a5298);
            color: #fff;
            overflow: hidden;
            -webkit-text-size-adjust: 100%;
            touch-action: manipulation;
            height: 100vh;
            display: flex;
            flex-direction: column;
        }
        
        #root {
            flex: 1;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 120px 20px 160px 20px;
        }
        
        .avatar {
            width: min(400px, 60vw);
            height: min(400px, 60vw);
            border-radius: 20px;
            background: rgba(255,255,255,0.1);
            display: flex;
            justify-content: center;
            align-items: center;
            transition: all 0.3s ease;
            box-shadow: 0 20px 40px rgba(0,0,0,0.3);
            position: relative;
            overflow: hidden;
            backdrop-filter: blur(10px);
            border: 2px solid rgba(255,255,255,0.2);
        }
        
        #avatar3d {
            width: 100%;
            height: 100%;
            display: block;
        }
        
        .avatar.speaking {
            /* No animation - 3D model handles talking animation */
        }
        .avatar.processing {
            /* No background change - using 3D model */
        }
        .avatar.active {
            /* No background change - using 3D model */
        }
        
        @keyframes float {
            0%, 100% { transform: translateY(0px); }
            50% { transform: translateY(-20px); }
        }
        @keyframes pulse {
            0% { transform: scale(1); }
            100% { transform: scale(1.1); }
        }
        
        .ui-overlay {
            position: fixed;
            top: 80px;
            left: 50%;
            transform: translateX(-50%);
            z-index: 100;
            display: flex;
            flex-direction: row;
            gap: 15px;
            width: calc(100% - 40px);
            max-width: 400px;
            justify-content: center;
        }
        
        button {
            padding: 16px 32px;
            background: #007bff;
            color: white;
            border: none;
            border-radius: 12px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
            min-height: 52px;
            min-width: 140px;
            touch-action: manipulation;
            user-select: none;
            transition: all 0.2s ease;
            box-shadow: 0 4px 12px rgba(0,123,255,0.3);
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        button:active {
            transform: scale(0.98);
            background: #0056b3;
        }
        
        button:hover {
            background: #0056b3;
            box-shadow: 0 6px 16px rgba(0,123,255,0.4);
        }
        
        button:disabled {
            background: #6c757d;
            cursor: not-allowed;
            opacity: 0.6;
            transform: none;
        }
        
        .chat-input {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            width: calc(100% - 40px);
            max-width: 600px;
        }
        
        input {
            flex: 1;
            padding: 18px 20px;
            border: 2px solid rgba(255,255,255,0.2);
            border-radius: 12px;
            font-size: 18px;
            background: rgba(255,255,255,0.1);
            color: white;
            backdrop-filter: blur(10px);
            min-height: 56px;
        }
        
        input::placeholder {
            color: rgba(255,255,255,0.7);
        }
        
        input:focus {
            outline: none;
            border-color: #007bff;
            background: rgba(255,255,255,0.15);
        }
        
        .send-btn {
            min-width: 80px;
            flex-shrink: 0;
        }
        
        .status {
            position: fixed;
            top: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0,0,0,0.8);
            padding: 15px 20px;
            border-radius: 12px;
            min-width: 160px;
            font-size: 16px;
            font-weight: 500;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255,255,255,0.1);
            text-align: center;
        }
        
        .response-display {
            position: fixed;
            bottom: 180px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0,0,0,0.9);
            padding: 20px 25px;
            border-radius: 16px;
            max-width: calc(100% - 40px);
            width: auto;
            max-width: 600px;
            display: none;
            font-size: 18px;
            line-height: 1.5;
            backdrop-filter: blur(15px);
            border: 1px solid rgba(255,255,255,0.1);
            box-shadow: 0 8px 24px rgba(0,0,0,0.4);
        }
        
        /* Mobile specific adjustments */
        @media (max-width: 768px) {
            #root {
                padding: 120px 20px 180px 20px;
            }
            
            .ui-overlay {
                top: 80px;
                gap: 12px;
                flex-direction: column;
                max-width: 300px;
            }
            
            button {
                padding: 14px 28px;
                font-size: 15px;
                min-height: 48px;
                width: 100%;
                min-width: 200px;
            }
            
            .chat-input {
                bottom: 15px;
                gap: 12px;
                width: calc(100% - 30px);
            }
            
            input {
                padding: 16px 18px;
                font-size: 16px;
                min-height: 52px;
            }
            
            .send-btn {
                min-width: 90px;
                font-size: 15px;
                padding: 14px 20px;
            }
            
            .status {
                top: 15px;
                padding: 12px 16px;
                font-size: 14px;
                min-width: 140px;
            }
            
            .response-display {
                bottom: 200px;
                padding: 18px 20px;
                font-size: 16px;
                max-width: calc(100% - 30px);
            }
            
            .avatar {
                width: min(300px, 50vw);
                height: min(300px, 50vw);
            }
        }
        
        /* Small mobile devices */
        @media (max-width: 480px) {
            #root {
                padding: 140px 20px 220px 20px;
            }
            
            .ui-overlay {
                top: 80px;
                max-width: 280px;
            }
            
            .chat-input {
                flex-direction: column;
                gap: 10px;
                bottom: 10px;
            }
            
            .send-btn {
                min-width: 100%;
                margin: 0;
                padding: 14px 20px;
                font-size: 15px;
            }
            
            .response-display {
                bottom: 240px;
            }
            
            .avatar {
                width: min(250px, 45vw);
                height: min(250px, 45vw);
            }
        }
        
        /* Landscape mobile */
        @media (max-height: 600px) and (orientation: landscape) {
            #root {
                padding: 80px 20px 80px 20px;
            }
            
            .avatar {
                width: min(200px, 35vh);
                height: min(200px, 35vh);
            }
            
            .ui-overlay {
                top: 50px;
                flex-direction: row;
                gap: 10px;
                max-width: 350px;
            }
            
            button {
                width: auto;
                min-width: 120px;
                padding: 12px 24px;
                font-size: 14px;
                min-height: 44px;
            }
            
            .response-display {
                bottom: 120px;
                font-size: 14px;
                padding: 12px 16px;
            }
            
            .chat-input {
                bottom: 10px;
            }
        }
    </style>
</head>
<body>
    <div id="root">
        <div class="avatar" id="avatar">
            <canvas id="avatar3d"></canvas>
        </div>
    </div>
    
    <div class="ui-overlay">
        <button id="startBtn">Start Chat</button>
        <button id="voiceBtn" disabled>🎤 Voice</button>
    </div>
    
    <div class="chat-input">
        <input type="text" id="messageInput" placeholder="Type your message..." />
        <button id="sendBtn">Send</button>
    </div>
    
    <div class="status" id="status">Ready</div>
    <div class="response-display" id="responseDisplay"></div>
    
    <script>
        const avatar = document.getElementById('avatar');
        const status = document.getElementById('status');
        const responseDisplay = document.getElementById('responseDisplay');
        const messageInput = document.getElementById('messageInput');
        
        let isActive = false;
        let isListening = false;
        
        // 3D Avatar System
        let scene, camera, renderer, avatarModel, mixer, clock;
        let mouseX = 0, mouseY = 0;
        let targetRotationX = 0, targetRotationY = 0;
        let isModelLoaded = false;
        
        // Initialize 3D Avatar
        function init3DAvatar() {
            const canvas = document.getElementById('avatar3d');
            const container = document.getElementById('avatar');
            
            // Scene setup
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(50, 1, 0.1, 1000);
            renderer = new THREE.WebGLRenderer({ 
                canvas: canvas, 
                alpha: true, 
                antialias: true 
            });
            
            const rect = container.getBoundingClientRect();
            renderer.setSize(rect.width, rect.height);
            renderer.setPixelRatio(window.devicePixelRatio);
            renderer.outputEncoding = THREE.sRGBEncoding;
            renderer.shadowMap.enabled = true;
            renderer.shadowMap.type = THREE.PCFSoftShadowMap;
            
            // Lighting
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.6);
            scene.add(ambientLight);
            
            const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
            directionalLight.position.set(1, 1, 1);
            directionalLight.castShadow = true;
            scene.add(directionalLight);
            
            const fillLight = new THREE.DirectionalLight(0x4a90e2, 0.3);
            fillLight.position.set(-1, 0, 1);
            scene.add(fillLight);
            
            // Camera position - positioned to show head and face
            camera.position.set(0, 3.0, 2.0);
            camera.lookAt(0, 3.0, 0);
            
            // Load 3D model
            const loader = new THREE.GLTFLoader();
            loader.load(
                'https://models.readyplayer.me/6864cd2efe9030c714ea760d.glb',
                function(gltf) {
                    avatarModel = gltf.scene;
                    
                    // Scale and position model to show head/face
                    avatarModel.scale.set(3.0, 3.0, 3.0);
                    avatarModel.position.set(0, -2.0, 0);
                    
                    // Enable shadows
                    avatarModel.traverse(function(child) {
                        if (child.isMesh) {
                            child.castShadow = true;
                            child.receiveShadow = true;
                            
                            // Debug: Check for morph targets on each mesh
                            if (child.morphTargetDictionary) {
                                console.log('🔍 Found mesh with morph targets:', child.name);
                                console.log('Available morph targets:', Object.keys(child.morphTargetDictionary));
                                
                                // Check specifically for viseme blend shapes
                                const visemeTargets = Object.keys(child.morphTargetDictionary).filter(key => 
                                    key.toLowerCase().includes('viseme') || 
                                    key.toLowerCase().includes('mouth') ||
                                    key.toLowerCase().includes('jaw')
                                );
                                
                                if (visemeTargets.length > 0) {
                                    console.log('✅ Found viseme targets:', visemeTargets);
                                } else {
                                    console.log('❌ No viseme targets found in this mesh');
                                }
                            }
                        }
                    });
                    
                    scene.add(avatarModel);
                    
                    // Animation mixer
                    if (gltf.animations && gltf.animations.length > 0) {
                        mixer = new THREE.AnimationMixer(avatarModel);
                        gltf.animations.forEach(clip => {
                            mixer.clipAction(clip);
                        });
                    }
                    
                    isModelLoaded = true;
                    console.log('3D Avatar loaded successfully');
                    
                    // Initialize blend shapes after model loads
                    findBones();
                },
                function(progress) {
                    console.log('Loading progress:', (progress.loaded / progress.total * 100) + '%');
                },
                function(error) {
                    console.error('Error loading 3D model:', error);
                    // Fallback to simple geometry
                    createFallbackAvatar();
                }
            );
            
            clock = new THREE.Clock();
            
            // Mouse tracking disabled
            // document.addEventListener('mousemove', onMouseMove);
            window.addEventListener('resize', onWindowResize);
            
            // Start animation loop
            animate();
        }
        
        function createFallbackAvatar() {
            const geometry = new THREE.SphereGeometry(0.3, 32, 32);
            const material = new THREE.MeshPhongMaterial({ 
                color: 0x4a90e2,
                transparent: true,
                opacity: 0.8
            });
            avatarModel = new THREE.Mesh(geometry, material);
            avatarModel.position.set(0, 0, 0);
            scene.add(avatarModel);
            isModelLoaded = true;
        }
        
        function onMouseMove(event) {
            if (!isModelLoaded) return;
            
            const rect = avatar.getBoundingClientRect();
            mouseX = ((event.clientX - rect.left) / rect.width) * 2 - 1;
            mouseY = -((event.clientY - rect.top) / rect.height) * 2 + 1;
            
            targetRotationY = mouseX * 0.3;
            targetRotationX = mouseY * 0.2;
        }
        
        function onWindowResize() {
            const container = document.getElementById('avatar');
            const rect = container.getBoundingClientRect();
            
            camera.aspect = rect.width / rect.height;
            camera.updateProjectionMatrix();
            renderer.setSize(rect.width, rect.height);
        }
        
        function animate() {
            requestAnimationFrame(animate);
            
            if (isModelLoaded && avatarModel) {
                // Idle breathing animation (only when not talking)
                if (!isTalking) {
                    const time = clock.getElapsedTime();
                    avatarModel.position.y = -2.0 + Math.sin(time * 2) * 0.02;
                }
                
                // Update animations
                if (mixer) {
                    mixer.update(clock.getDelta());
                }
            }
            
            renderer.render(scene, camera);
        }
        
        let isTalking = false;
        let talkingAnimation = null;
        let headBone = null;
        let jawBone = null;
        let faceMesh = null;
        let smileBlendShape = null;
        let mouthShapes = {};
        let currentAudio = null;
        let isPlaying = false;
        
        // AWS Polly viseme mapping for basic mouth shapes (mouthOpen, mouthSmile)
        const visemeToMouthShape = {
            // Silence
            'sil': { open: 0.0, smile: 0.0 },
            
            // Consonants - lowercase as returned by AWS Polly
            'p': { open: 0.0, smile: 0.0 },     // P, B, M - lips closed
            'f': { open: 0.1, smile: 0.0 },     // F, V - lips slightly open
            't': { open: 0.3, smile: 0.0 },     // T, D, N, L - tongue tip to roof
            'S': { open: 0.1, smile: 0.0 },     // S, Z - tongue tip down (uppercase S)
            's': { open: 0.1, smile: 0.0 },     // S, Z - tongue tip down (lowercase s)
            'r': { open: 0.2, smile: 0.0 },     // R - tongue pulled back
            'k': { open: 0.2, smile: 0.0 },     // K, G - back of tongue up
            
            // Vowels - lowercase as returned by AWS Polly
            'a': { open: 0.8, smile: 0.0 },     // AH, AA - wide open
            'e': { open: 0.4, smile: 0.3 },     // EH, EY - medium open, slight smile
            'i': { open: 0.2, smile: 0.5 },     // IH, IY - small opening, smile
            'o': { open: 0.6, smile: 0.0 },     // OH, OW - round mouth
            'u': { open: 0.3, smile: 0.0 },     // UH, UW - small round opening
            '@': { open: 0.5, smile: 0.0 },     // Schwa - medium open
            
            // Uppercase versions (AWS Polly sometimes uses these)
            'PP': { open: 0.0, smile: 0.0 },
            'FF': { open: 0.1, smile: 0.0 },
            'TH': { open: 0.2, smile: 0.0 },
            'DD': { open: 0.3, smile: 0.0 },
            'kk': { open: 0.2, smile: 0.0 },
            'CH': { open: 0.1, smile: 0.0 },
            'SS': { open: 0.1, smile: 0.0 },
            'nn': { open: 0.1, smile: 0.0 },
            'RR': { open: 0.2, smile: 0.0 },
            'aa': { open: 0.8, smile: 0.0 },
            'E': { open: 0.4, smile: 0.3 },
            'I': { open: 0.2, smile: 0.5 },
            'O': { open: 0.6, smile: 0.0 },
            'U': { open: 0.3, smile: 0.0 },
            
            // Special cases
            'T': { open: 0.3, smile: 0.0 }      // Sometimes uppercase T
        };
        
        function setAvatarState(state) {
            avatar.className = 'avatar';
            
            if (!isModelLoaded) return;
            
            // Find facial features if not already found
            if (!faceMesh) {
                findBones();
            }
            
            switch(state) {
                case 'active':
                    avatar.className = 'avatar active';
                    stopTalking();
                    setSmile(0.6); // Happy smile when active
                    break;
                case 'processing':
                    avatar.className = 'avatar processing';
                    stopTalking();
                    clearSmile(); // Neutral while thinking
                    break;
                case 'speaking':
                    avatar.className = 'avatar speaking';
                    startTalking();
                    setSmile(0.3); // Slight smile while talking
                    break;
                case 'listening':
                    avatar.className = 'avatar processing';
                    stopTalking();
                    setSmile(0.4); // Attentive smile while listening
                    break;
                default:
                    stopTalking();
                    clearSmile();
                    break;
            }
        }
        
        function findBones() {
            if (!avatarModel) return;
            
            // Try to find head, jaw bones and facial blend shapes
            avatarModel.traverse(function(child) {
                // Look for bones
                if (child.isBone || child.name) {
                    const name = child.name.toLowerCase();
                    if (name.includes('head') && !headBone) {
                        headBone = child;
                        console.log('Found head bone:', child.name);
                    }
                    if (name.includes('jaw') && !jawBone) {
                        jawBone = child;
                        console.log('Found jaw bone:', child.name);
                    }
                }
                
                // Look for facial mesh with morph targets
                if (child.isMesh && child.morphTargetDictionary) {
                    faceMesh = child;
                    console.log('Found face mesh with morph targets:', Object.keys(child.morphTargetDictionary));
                    
                    const morphTargets = child.morphTargetDictionary;
                    
                    // Look for smile-related blend shapes
                    for (let target in morphTargets) {
                        const targetName = target.toLowerCase();
                        if (targetName.includes('smile') || targetName.includes('happy') || targetName.includes('joy')) {
                            smileBlendShape = { mesh: child, index: morphTargets[target] };
                            console.log('Found smile blend shape:', target, 'at index', morphTargets[target]);
                            break;
                        }
                    }
                    
                    // Look for basic mouth shapes (mouthOpen, mouthSmile)
                    const hasMouthShapes = morphTargets.hasOwnProperty('mouthOpen') || 
                                         morphTargets.hasOwnProperty('mouthSmile') ||
                                         Object.keys(morphTargets).some(key => key.toLowerCase().includes('mouth'));
                    
                    if (hasMouthShapes) {
                        mouthShapes[child.uuid] = {
                            mesh: child,
                            morphTargets: morphTargets
                        };
                        console.log(`✅ Found mouth shapes for lip sync on ${child.name}:`, 
                                  Object.keys(morphTargets).filter(key => key.toLowerCase().includes('mouth')));
                    }
                }
            });
            
            if (Object.keys(mouthShapes).length > 0) {
                console.log(`🎉 Lip sync ready! Found ${Object.keys(mouthShapes).length} meshes with mouth shapes`);
            } else {
                console.log('❌ No mouth shapes found for lip sync');
            }
        }
        
        // Set viseme using basic mouth shapes (mouthOpen, mouthSmile)
        function setViseme(viseme, weight = 1.0) {
            console.log(`🗣️ setViseme called: ${viseme} with weight ${weight}`);
            
            if (!avatarModel) {
                console.log('❌ No avatar model loaded');
                return;
            }
            
            if (Object.keys(mouthShapes).length === 0) {
                console.log('❌ No mouth shapes found');
                return;
            }
            
            const mouthShape = visemeToMouthShape[viseme];
            if (!mouthShape) {
                console.log(`❌ No mouth shape mapping for viseme: ${viseme}`);
                return;
            }
            
            console.log(`🎯 Setting mouth: open=${mouthShape.open}, smile=${mouthShape.smile}`);
            
            // Apply mouth shapes to all meshes that have them
            let shapesApplied = 0;
            Object.values(mouthShapes).forEach(shapeData => {
                const morphTargets = shapeData.morphTargets;
                const mesh = shapeData.mesh;
                
                // Set mouthOpen
                if (morphTargets.hasOwnProperty('mouthOpen')) {
                    const openIndex = morphTargets['mouthOpen'];
                    mesh.morphTargetInfluences[openIndex] = mouthShape.open * weight;
                    shapesApplied++;
                    console.log(`✅ Set mouthOpen to ${mouthShape.open * weight} on ${mesh.name}`);
                }
                
                // Set mouthSmile
                if (morphTargets.hasOwnProperty('mouthSmile')) {
                    const smileIndex = morphTargets['mouthSmile'];
                    mesh.morphTargetInfluences[smileIndex] = mouthShape.smile * weight;
                    shapesApplied++;
                    console.log(`✅ Set mouthSmile to ${mouthShape.smile * weight} on ${mesh.name}`);
                }
            });
            
            if (shapesApplied === 0) {
                console.log(`❌ No mouth shapes applied`);
            } else {
                console.log(`✅ Applied ${shapesApplied} mouth shape changes`);
            }
        }
        
        // Play audio with lip sync animation
        async function playAudioWithLipSync(audioDataUrl, visemes) {
            return new Promise((resolve, reject) => {
                // Create audio element
                currentAudio = new Audio(audioDataUrl);
                isPlaying = true;
                
                let visemeIndex = 0;
                let animationId = null;
                let audioStartTime = null;
                
                console.log(`🎬 Starting lip sync with ${visemes.length} visemes`);
                
                const animateLipSync = () => {
                    if (!isPlaying || !currentAudio) {
                        console.log('🛑 Stopping lip sync animation');
                        return;
                    }
                    
                    // Use audio's currentTime for more accurate sync
                    const elapsed = currentAudio.currentTime;
                    
                    // Find the current viseme based on audio time
                    let targetIndex = visemeIndex;
                    
                    // Look ahead to find the right viseme for current time
                    for (let i = 0; i < visemes.length; i++) {
                        if (visemes[i].time <= elapsed) {
                            targetIndex = i;
                        } else {
                            break;
                        }
                    }
                    
                    // Only update if we've moved to a new viseme
                    if (targetIndex !== visemeIndex) {
                        visemeIndex = targetIndex;
                        const currentViseme = visemes[visemeIndex];
                        setViseme(currentViseme.viseme);
                        console.log(`🎵 Lip sync: ${currentViseme.viseme} at ${elapsed.toFixed(2)}s`);
                    }
                    
                    // Continue animation while audio is playing
                    if (!currentAudio.paused && !currentAudio.ended) {
                        animationId = requestAnimationFrame(animateLipSync);
                    } else {
                        console.log('🏁 Audio ended or paused, stopping lip sync');
                        isPlaying = false;
                        setViseme('sil');
                    }
                };
                
                currentAudio.onloadeddata = () => {
                    console.log('📱 Audio loaded, duration:', currentAudio.duration);
                };
                
                currentAudio.onplay = () => {
                    console.log('▶️ Audio started playing');
                    audioStartTime = Date.now();
                    animationId = requestAnimationFrame(animateLipSync);
                };
                
                currentAudio.onended = () => {
                    console.log('🏁 Audio playback ended');
                    isPlaying = false;
                    if (animationId) {
                        cancelAnimationFrame(animationId);
                    }
                    setViseme('sil'); // Reset to silent
                    resolve();
                };
                
                currentAudio.onpause = () => {
                    console.log('⏸️ Audio paused');
                    isPlaying = false;
                    if (animationId) {
                        cancelAnimationFrame(animationId);
                    }
                };
                
                currentAudio.onerror = (error) => {
                    console.error('🚨 Audio playback error:', error);
                    isPlaying = false;
                    if (animationId) {
                        cancelAnimationFrame(animationId);
                    }
                    setViseme('sil');
                    reject(error);
                };
                
                // Start playing
                console.log('🚀 Starting audio playback...');
                currentAudio.play().catch(error => {
                    console.error('🚨 Failed to start audio:', error);
                    reject(error);
                });
            });
        }
        
        function setSmile(intensity = 0.5) {
            if (smileBlendShape && smileBlendShape.mesh.morphTargetInfluences) {
                smileBlendShape.mesh.morphTargetInfluences[smileBlendShape.index] = Math.max(0, Math.min(1, intensity));
                console.log('Setting smile intensity:', intensity);
            }
        }
        
        function clearSmile() {
            if (smileBlendShape && smileBlendShape.mesh.morphTargetInfluences) {
                smileBlendShape.mesh.morphTargetInfluences[smileBlendShape.index] = 0;
            }
        }
        
        function startTalking() {
            if (isTalking) return;
            isTalking = true;
            
            console.log('Starting talking animation');
            
            // Simple talking animation - only head rotation, no position changes
            function talkingLoop() {
                if (!isTalking || !avatarModel) return;
                
                const time = performance.now() * 0.003;
                
                // Continuous gentle head bobbing - rotation only
                avatarModel.rotation.y = Math.sin(time) * 0.015;
                avatarModel.rotation.x = Math.sin(time * 0.8) * 0.01;
                avatarModel.rotation.z = Math.sin(time * 1.2) * 0.005;
                
                // NO position changes - keep avatar container stable
                
                talkingAnimation = requestAnimationFrame(talkingLoop);
            }
            
            talkingLoop();
        }
        
        function stopTalking() {
            isTalking = false;
            console.log('Stopping talking animation');
            
            if (talkingAnimation) {
                cancelAnimationFrame(talkingAnimation);
                talkingAnimation = null;
            }
            
            // Reset to neutral position smoothly - no position changes
            if (avatarModel) {
                avatarModel.rotation.x = 0;
                avatarModel.rotation.y = 0;
                avatarModel.rotation.z = 0;
                // Keep position stable at -2.0, no breathing movement
            }
        }
        
        // Test function for manual viseme testing (call from console)
        window.testViseme = function(viseme = 'aa') {
            console.log('🧪 Manual viseme test:', viseme);
            setViseme(viseme, 1.0);
            setTimeout(() => {
                setViseme('sil', 0);
                console.log('🧪 Reset to silent');
            }, 2000);
        };
        
        // Test function to show all available blend shapes
        window.showBlendShapes = function() {
            if (!avatarModel) {
                console.log('❌ No avatar loaded');
                return;
            }
            
            avatarModel.traverse(function(child) {
                if (child.isMesh && child.morphTargetDictionary) {
                    console.log(`Mesh: ${child.name || 'unnamed'}`);
                    console.log('All blend shapes:', Object.keys(child.morphTargetDictionary));
                }
            });
        };

        // Initialize 3D avatar when page loads
        window.addEventListener('load', init3DAvatar);
        
        // Start chat
        document.getElementById('startBtn').addEventListener('click', () => {
            isActive = true;
            status.textContent = 'Chat Active';
            document.getElementById('voiceBtn').disabled = false;
            setAvatarState('active');
            
            // No automatic speech - just activate quietly
            
            setTimeout(() => {
                setAvatarState('idle');
            }, 2000);
        });
        
        // Send message
        document.getElementById('sendBtn').addEventListener('click', async () => {
            const message = messageInput.value.trim();
            
            if (!message) return;
            
            if (!isActive) {
                alert('Please click "Start Chat" first!');
                return;
            }
            
            // Show processing
            status.textContent = 'Processing...';
            setAvatarState('processing');
            messageInput.value = '';
            responseDisplay.style.display = 'none';
            
            try {
                const response = await fetch('/api/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({ message })
                });
                
                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }
                
                const data = await response.json();
                const aiResponse = data.response;
                
                // Show response
                responseDisplay.textContent = aiResponse;
                responseDisplay.style.display = 'block';
                
                // Try to generate speech with AWS Polly (male voice)
                try {
                    status.textContent = 'Generating speech...';
                    console.log('Requesting AWS Polly with male voice:', { 
                        text: aiResponse,
                        voice: 'Matthew',
                        gender: 'male'
                    });
                    
                    const audioResponse = await fetch('/api/speak', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify({ 
                            text: aiResponse,
                            voice: 'Brian',          // Try Brian (British), Joey, or Russell
                            voiceId: 'Matthew',      // Alternative parameter name
                            VoiceId: 'Matthew',      // AWS API format
                            gender: 'male',
                            // Try multiple male voices
                            Engine: 'standard'
                        })
                    });
                    
                    if (audioResponse.ok) {
                        const contentType = audioResponse.headers.get('content-type');
                        
                        // Check if it's JSON response (new format with visemes)
                        if (contentType && contentType.includes('application/json')) {
                            const audioData = await audioResponse.json();
                            
                            // Check if we have viseme data for lip sync
                            if (audioData.audio && audioData.visemes) {
                                console.log('Playing AWS Polly with lip sync - visemes:', audioData.visemes.length);
                                
                                status.textContent = 'Speaking with lip sync...';
                                setAvatarState('speaking');
                                
                                try {
                                    // Play audio with lip sync
                                    await playAudioWithLipSync(audioData.audio, audioData.visemes);
                                    
                                    status.textContent = 'Ready';
                                    setAvatarState('idle');
                                    
                                    setTimeout(() => {
                                        responseDisplay.style.display = 'none';
                                    }, 3000);
                                    
                                } catch (error) {
                                    console.error('Lip sync playback failed:', error);
                                    fallbackToTextDisplay();
                                }
                                
                            } else if (audioData.useBrowserTTS) {
                                // Handle browser TTS fallback with male voice
                                console.log('Using browser Speech Synthesis API with male voice');
                                status.textContent = 'Speaking...';
                                setAvatarState('speaking');
                                
                                if ('speechSynthesis' in window) {
                                    const utterance = new SpeechSynthesisUtterance(audioData.text);
                                    
                                    // Set male voice - prioritize Eddy on macOS
                                    const voices = speechSynthesis.getVoices();
                                    const maleVoice = voices.find(voice => 
                                        voice.name.toLowerCase().includes('eddy') ||         // Preferred male voice on Mac
                                        voice.name.toLowerCase().includes('fred') ||        // Deep male voice on Mac
                                        voice.name.toLowerCase().includes('daniel') ||      // Male voice on Mac  
                                        voice.name.toLowerCase().includes('david') ||       // Windows male
                                        voice.name.toLowerCase().includes('mark') ||        // Windows male
                                        voice.name.toLowerCase().includes('richard') ||     // Windows male
                                        voice.name.toLowerCase().includes('google uk english male') ||
                                        voice.name.toLowerCase().includes('google us english male') ||
                                        (voice.lang.startsWith('en') && voice.gender === 'male')
                                    );
                                    
                                    if (maleVoice) {
                                        utterance.voice = maleVoice;
                                        console.log('Using male voice:', maleVoice.name);
                                    } else {
                                        console.log('No specific male voice found, using default');
                                    }
                                    
                                    utterance.rate = 0.85; // Slightly slower for deeper sound
                                    utterance.pitch = 0.6; // Much lower pitch for masculine sound
                                    utterance.volume = 0.8;
                                    
                                    utterance.onend = () => {
                                        status.textContent = 'Ready';
                                        setAvatarState('idle');
                                        
                                        setTimeout(() => {
                                            responseDisplay.style.display = 'none';
                                        }, 3000);
                                    };
                                    
                                    utterance.onerror = () => {
                                        console.error('Browser TTS failed');
                                        fallbackToTextDisplay();
                                    };
                                    
                                    speechSynthesis.speak(utterance);
                                } else {
                                    fallbackToTextDisplay();
                                }
                            } else {
                                fallbackToTextDisplay();
                            }
                            
                        } else if (contentType && contentType.includes('audio')) {
                            // Legacy audio format (no visemes)
                            console.log('Playing AWS Polly audio (legacy format)');
                            const audioBlob = await audioResponse.blob();
                            const audioUrl = URL.createObjectURL(audioBlob);
                            const audio = new Audio(audioUrl);
                            
                            status.textContent = 'Speaking...';
                            setAvatarState('speaking');
                            
                            audio.play().catch(error => {
                                console.error('Audio play failed:', error);
                                fallbackToTextDisplay();
                            });
                            
                            audio.onended = () => {
                                status.textContent = 'Ready';
                                setAvatarState('idle');
                                URL.revokeObjectURL(audioUrl);
                                
                                setTimeout(() => {
                                    responseDisplay.style.display = 'none';
                                }, 3000);
                            };
                            
                            audio.onerror = () => {
                                console.error('Audio playback failed');
                                fallbackToTextDisplay();
                            };
                            
                        } else {
                            fallbackToTextDisplay();
                        }
                        
                    } else {
                        console.log('TTS not available, showing text only');
                        fallbackToTextDisplay();
                    }
                    
                } catch (ttsError) {
                    console.error('TTS error:', ttsError);
                    fallbackToTextDisplay();
                }
                
                function fallbackToTextDisplay() {
                    status.textContent = 'Speaking...';
                    setAvatarState('speaking');
                    
                    const speakingTime = Math.max(2000, aiResponse.length * 50);
                    setTimeout(() => {
                        status.textContent = 'Ready';
                        setAvatarState('idle');
                        
                        setTimeout(() => {
                            responseDisplay.style.display = 'none';
                        }, 3000);
                    }, speakingTime);
                }
                
            } catch (error) {
                console.error('Chat error:', error);
                status.textContent = 'Error - using fallback';
                
                // Fallback responses
                const fallbackResponses = [
                    "Hello! How can I help you today?",
                    "That's interesting! Tell me more.",
                    "I understand what you're saying.",
                    "Great question! Let me think about that.",
                    "I'm here to help with whatever you need."
                ];
                
                const response = fallbackResponses[Math.floor(Math.random() * fallbackResponses.length)];
                responseDisplay.textContent = response;
                responseDisplay.style.display = 'block';
                setAvatarState('speaking');
                
                setTimeout(() => {
                    status.textContent = 'Ready';
                    setAvatarState('idle');
                    setTimeout(() => {
                        responseDisplay.style.display = 'none';
                    }, 3000);
                }, 2000);
            }
        });
        
        // Voice input
        document.getElementById('voiceBtn').addEventListener('click', () => {
            if (!isActive) {
                alert('Please click "Start Chat" first!');
                return;
            }
            
            if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                const recognition = new SpeechRecognition();
                
                recognition.continuous = false;
                recognition.interimResults = false;
                recognition.lang = 'en-US';
                
                if (!isListening) {
                    recognition.start();
                    isListening = true;
                    document.getElementById('voiceBtn').textContent = '🔴 Listening...';
                    status.textContent = 'Listening...';
                    setAvatarState('listening');
                }
                
                recognition.onresult = (event) => {
                    const transcript = event.results[0][0].transcript;
                    messageInput.value = transcript;
                    document.getElementById('sendBtn').click();
                };
                
                recognition.onend = () => {
                    isListening = false;
                    document.getElementById('voiceBtn').textContent = '🎤 Voice';
                    if (status.textContent === 'Listening...') {
                        status.textContent = 'Ready';
                        setAvatarState('idle');
                    }
                };
                
                recognition.onerror = (event) => {
                    console.error('Speech recognition error:', event.error);
                    isListening = false;
                    document.getElementById('voiceBtn').textContent = '🎤 Voice';
                    status.textContent = 'Speech error - try typing';
                    setAvatarState('idle');
                };
            } else {
                alert('Speech recognition not supported in this browser. Try Chrome or Edge.');
            }
        });
        
        // Enter key support
        messageInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                document.getElementById('sendBtn').click();
            }
        });
    </script>
</body>
</html>